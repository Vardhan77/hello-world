{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "densenet for cifar 10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/Vardhan77/hello-world/blob/master/densenet_for_cifar_10.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "viu6-wO2UrXx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d7cc97e3-b270-4b1c-cefa-426411a5e383"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HWHPmGmxZFjP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filepath=\"/content/gdrive/My Drive/model\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1mTkObZ_lmu9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3451d6d7-8b6f-41e1-fca0-5a5c2f6b8729"
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# 2. Save Keras Model or weights on google drive\n",
        "\n",
        "# create on Colab directory\n",
        "model.save('model.h5')    \n",
        "model_file = drive.CreateFile({'title' : 'model.h5'})\n",
        "model_file.SetContentFile('model.h5')\n",
        "model_file.Upload()\n",
        "\n",
        "# download to google drive\n",
        "drive.CreateFile({'id': model_file.get('id')})"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GoogleDriveFile({'id': '1c6KYo3IWG1h3TxikVIea8h5TnmW_0ZMr'})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "mcl6aB1d1T5Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "28922e60-85c5-4820-c238-15917d8c7a44"
      },
      "cell_type": "code",
      "source": [
        "!pip install keras\n",
        "import keras\n",
        "from __future__ import absolute_import\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.engine import Layer\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "#from K import normalize_data_format\n",
        "#self.data_format = K.normalize_data_format(data_format)\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "#if K.backend() == 'theano':\n",
        "#    import theano_backend as K_BACKEND\n",
        "#else:\n",
        "#from keras.backend.tensorflow_backend import tensorflow_backend as K_BACKEND\n",
        "\n",
        "# Don't pre-allocate memory; allocate as-needed\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "# Create a session with the above options specified.\n",
        "K.tensorflow_backend.set_session(tf.Session(config=config))\n",
        "\n",
        "\n",
        "class SubPixelUpscaling(Layer):\n",
        "    \"\"\" Sub-pixel convolutional upscaling layer based on the paper \"Real-Time Single Image\n",
        "    and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network\"\n",
        "    (https://arxiv.org/abs/1609.05158).\n",
        "    This layer requires a Convolution2D prior to it, having output filters computed according to\n",
        "    the formula :\n",
        "        filters = k * (scale_factor * scale_factor)\n",
        "        where k = a user defined number of filters (generally larger than 32)\n",
        "              scale_factor = the upscaling factor (generally 2)\n",
        "    This layer performs the depth to space operation on the convolution filters, and returns a\n",
        "    tensor with the size as defined below.\n",
        "    # Example :\n",
        "    ```python\n",
        "        # A standard subpixel upscaling block\n",
        "        x = Convolution2D(256, 3, 3, padding='same', activation='relu')(...)\n",
        "        u = SubPixelUpscaling(scale_factor=2)(x)\n",
        "        [Optional]\n",
        "        x = Convolution2D(256, 3, 3, padding='same', activation='relu')(u)\n",
        "    ```\n",
        "        In practice, it is useful to have a second convolution layer after the\n",
        "        SubPixelUpscaling layer to speed up the learning process.\n",
        "        However, if you are stacking multiple SubPixelUpscaling blocks, it may increase\n",
        "        the number of parameters greatly, so the Convolution layer after SubPixelUpscaling\n",
        "        layer can be removed.\n",
        "    # Arguments\n",
        "        scale_factor: Upscaling factor.\n",
        "        data_format: Can be None, 'channels_first' or 'channels_last'.\n",
        "    # Input shape\n",
        "        4D tensor with shape:\n",
        "        `(samples, k * (scale_factor * scale_factor) channels, rows, cols)` if data_format='channels_first'\n",
        "        or 4D tensor with shape:\n",
        "        `(samples, rows, cols, k * (scale_factor * scale_factor) channels)` if data_format='channels_last'.\n",
        "    # Output shape\n",
        "        4D tensor with shape:\n",
        "        `(samples, k channels, rows * scale_factor, cols * scale_factor))` if data_format='channels_first'\n",
        "        or 4D tensor with shape:\n",
        "        `(samples, rows * scale_factor, cols * scale_factor, k channels)` if data_format='channels_last'.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, scale_factor=2, data_format=None, **kwargs):\n",
        "        super(SubPixelUpscaling, self).__init__(**kwargs)\n",
        "\n",
        "        self.scale_factor = scale_factor\n",
        "        self.data_format = normalize_data_format(data_format)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        y = K_BACKEND.depth_to_space(x, self.scale_factor, self.data_format)\n",
        "        return y\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if self.data_format == 'channels_first':\n",
        "            b, k, r, c = input_shape\n",
        "            return (b, k // (self.scale_factor ** 2), r * self.scale_factor, c * self.scale_factor)\n",
        "        else:\n",
        "            b, r, c, k = input_shape\n",
        "            return (b, r * self.scale_factor, c * self.scale_factor, k // (self.scale_factor ** 2))\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'scale_factor': self.scale_factor,\n",
        "                  'data_format': self.data_format}\n",
        "        base_config = super(SubPixelUpscaling, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "get_custom_objects().update({'SubPixelUpscaling': SubPixelUpscaling})"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.1.6)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.11.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.14.6)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (0.19.1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "A8mexbdlgGEF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 12701
        },
        "outputId": "19ab520b-a27c-4187-f77f-f7b938006e5f"
      },
      "cell_type": "code",
      "source": [
        "'''DenseNet models for Keras.\n",
        "# Reference\n",
        "- [Densely Connected Convolutional Networks](https://arxiv.org/pdf/1608.06993.pdf)\n",
        "- [The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation](https://arxiv.org/pdf/1611.09326.pdf)\n",
        "'''\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "\n",
        "import warnings\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers.core import Dense, Dropout, Activation, Reshape\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose, UpSampling2D\n",
        "from keras.layers.pooling import AveragePooling2D, MaxPooling2D\n",
        "from keras.layers.pooling import GlobalAveragePooling2D\n",
        "from keras.layers import Input\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.utils.layer_utils import convert_all_kernels_in_model, convert_dense_weights_data_format\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.engine.topology import get_source_inputs\n",
        "from keras_applications.imagenet_utils import _obtain_input_shape\n",
        "from keras_applications.imagenet_utils import decode_predictions\n",
        "import keras.backend as K\n",
        "\n",
        "#from subpixel import SubPixelUpscaling\n",
        "\n",
        "#DENSENET_121_WEIGHTS_PATH = r'https://github.com/titu1994/DenseNet/releases/download/v3.0/DenseNet-BC-121-32.h5'\n",
        "#DENSENET_161_WEIGHTS_PATH = r'https://github.com/titu1994/DenseNet/releases/download/v3.0/DenseNet-BC-161-48.h5'\n",
        "#DENSENET_169_WEIGHTS_PATH = r'https://github.com/titu1994/DenseNet/releases/download/v3.0/DenseNet-BC-169-32.h5'\n",
        "#DENSENET_121_WEIGHTS_PATH_NO_TOP = r'https://github.com/titu1994/DenseNet/releases/download/v3.0/DenseNet-BC-121-32-no-top.h5'\n",
        "#DENSENET_161_WEIGHTS_PATH_NO_TOP = r'https://github.com/titu1994/DenseNet/releases/download/v3.0/DenseNet-BC-161-48-no-top.h5'\n",
        "#DENSENET_169_WEIGHTS_PATH_NO_TOP = r'https://github.com/titu1994/DenseNet/releases/download/v3.0/DenseNet-BC-169-32-no-top.h5'\n",
        "\n",
        "def preprocess_input(x, data_format=None):\n",
        "    \"\"\"Preprocesses a tensor encoding a batch of images.\n",
        "    # Arguments\n",
        "        x: input Numpy tensor, 4D.\n",
        "        data_format: data format of the image tensor.\n",
        "    # Returns\n",
        "        Preprocessed tensor.\n",
        "    \"\"\"\n",
        "    if data_format is None:\n",
        "        data_format = K.image_data_format()\n",
        "    assert data_format in {'channels_last', 'channels_first'}\n",
        "\n",
        "    if data_format == 'channels_first':\n",
        "        if x.ndim == 3:\n",
        "            # 'RGB'->'BGR'\n",
        "            x = x[::-1, ...]\n",
        "            # Zero-center by mean pixel\n",
        "            x[0, :, :] -= 103.939\n",
        "            x[1, :, :] -= 116.779\n",
        "            x[2, :, :] -= 123.68\n",
        "        else:\n",
        "            x = x[:, ::-1, ...]\n",
        "            x[:, 0, :, :] -= 103.939\n",
        "            x[:, 1, :, :] -= 116.779\n",
        "            x[:, 2, :, :] -= 123.68\n",
        "    else:\n",
        "        # 'RGB'->'BGR'\n",
        "        x = x[..., ::-1]\n",
        "        # Zero-center by mean pixel\n",
        "        x[..., 0] -= 103.939\n",
        "        x[..., 1] -= 116.779\n",
        "        x[..., 2] -= 123.68\n",
        "\n",
        "    x *= 0.017 # scale values\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def DenseNet(input_shape=None, depth=40, nb_dense_block=3, growth_rate=12, nb_filter=-1, nb_layers_per_block=-1,\n",
        "             bottleneck=False, reduction=0.0, dropout_rate=0.0, weight_decay=1e-4, subsample_initial_block=False,\n",
        "             include_top=True, weights=None, input_tensor=None,\n",
        "             classes=10, activation='softmax'):\n",
        "    '''Instantiate the DenseNet architecture,\n",
        "        optionally loading weights pre-trained\n",
        "        on CIFAR-10. Note that when using TensorFlow,\n",
        "        for best performance you should set\n",
        "        `image_data_format='channels_last'` in your Keras config\n",
        "        at ~/.keras/keras.json.\n",
        "        The model and the weights are compatible with both\n",
        "        TensorFlow and Theano. The dimension ordering\n",
        "        convention used by the model is the one\n",
        "        specified in your Keras config file.\n",
        "        # Arguments\n",
        "            input_shape: optional shape tuple, only to be specified\n",
        "                if `include_top` is False (otherwise the input shape\n",
        "                has to be `(32, 32, 3)` (with `channels_last` dim ordering)\n",
        "                or `(3, 32, 32)` (with `channels_first` dim ordering).\n",
        "                It should have exactly 3 inputs channels,\n",
        "                and width and height should be no smaller than 8.\n",
        "                E.g. `(200, 200, 3)` would be one valid value.\n",
        "            depth: number or layers in the DenseNet\n",
        "            nb_dense_block: number of dense blocks to add to end (generally = 3)\n",
        "            growth_rate: number of filters to add per dense block\n",
        "            nb_filter: initial number of filters. -1 indicates initial\n",
        "                number of filters is 2 * growth_rate\n",
        "            nb_layers_per_block: number of layers in each dense block.\n",
        "                Can be a -1, positive integer or a list.\n",
        "                If -1, calculates nb_layer_per_block from the network depth.\n",
        "                If positive integer, a set number of layers per dense block.\n",
        "                If list, nb_layer is used as provided. Note that list size must\n",
        "                be (nb_dense_block + 1)\n",
        "            bottleneck: flag to add bottleneck blocks in between dense blocks\n",
        "            reduction: reduction factor of transition blocks.\n",
        "                Note : reduction value is inverted to compute compression.\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay rate\n",
        "            subsample_initial_block: Set to True to subsample the initial convolution and\n",
        "                add a MaxPool2D before the dense blocks are added.\n",
        "            include_top: whether to include the fully-connected\n",
        "                layer at the top of the network.\n",
        "            weights: one of `None` (random initialization) or\n",
        "                'imagenet' (pre-training on ImageNet)..\n",
        "            input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n",
        "                to use as image input for the model.\n",
        "            classes: optional number of classes to classify images\n",
        "                into, only to be specified if `include_top` is True, and\n",
        "                if no `weights` argument is specified.\n",
        "            activation: Type of activation at the top layer. Can be one of 'softmax' or 'sigmoid'.\n",
        "                Note that if sigmoid is used, classes must be 1.\n",
        "        # Returns\n",
        "            A Keras model instance.\n",
        "        '''\n",
        "\n",
        "    if weights not in {'imagenet', None}:\n",
        "        raise ValueError('The `weights` argument should be either '\n",
        "                         '`None` (random initialization) or `cifar10` '\n",
        "                         '(pre-training on CIFAR-10).')\n",
        "\n",
        "    if weights == 'imagenet' and include_top and classes != 1000:\n",
        "        raise ValueError('If using `weights` as ImageNet with `include_top`'\n",
        "                         ' as true, `classes` should be 1000')\n",
        "\n",
        "    if activation not in ['softmax', 'sigmoid']:\n",
        "        raise ValueError('activation must be one of \"softmax\" or \"sigmoid\"')\n",
        "\n",
        "    if activation == 'sigmoid' and classes != 1:\n",
        "        raise ValueError('sigmoid activation can only be used when classes = 1')\n",
        "\n",
        "    # Determine proper input shape\n",
        "    input_shape = _obtain_input_shape(input_shape,\n",
        "                                      default_size=32,\n",
        "                                      min_size=8,\n",
        "                                      data_format=K.image_data_format(),\n",
        "                                      require_flatten=include_top)\n",
        "\n",
        "    if input_tensor is None:\n",
        "        img_input = Input(shape=input_shape)\n",
        "    else:\n",
        "        if not K.is_keras_tensor(input_tensor):\n",
        "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
        "        else:\n",
        "            img_input = input_tensor\n",
        "\n",
        "    x = __create_dense_net(classes, img_input, include_top, depth, nb_dense_block,\n",
        "                           growth_rate, nb_filter, nb_layers_per_block, bottleneck, reduction,\n",
        "                           dropout_rate, weight_decay, subsample_initial_block, activation)\n",
        "\n",
        "    # Ensure that the model takes into account\n",
        "    # any potential predecessors of `input_tensor`.\n",
        "    if input_tensor is not None:\n",
        "        inputs = get_source_inputs(input_tensor)\n",
        "    else:\n",
        "        inputs = img_input\n",
        "    # Create model.\n",
        "    model = Model(inputs, x, name='densenet')\n",
        "\n",
        "    # load weights\n",
        "    \"\"\"if weights == 'imagenet':\n",
        "        weights_loaded = False\n",
        "\n",
        "        if (depth == 121) and (nb_dense_block == 4) and (growth_rate == 32) and (nb_filter == 64) and \\\n",
        "                (bottleneck is True) and (reduction == 0.5) and (dropout_rate == 0.0) and (subsample_initial_block):\n",
        "            if include_top:\n",
        "                weights_path = get_file('DenseNet-BC-121-32.h5',\n",
        "                                        DENSENET_121_WEIGHTS_PATH,\n",
        "                                        cache_subdir='models',\n",
        "                                        md5_hash='a439dd41aa672aef6daba4ee1fd54abd')\n",
        "            else:\n",
        "                weights_path = get_file('DenseNet-BC-121-32-no-top.h5',\n",
        "                                        DENSENET_121_WEIGHTS_PATH_NO_TOP,\n",
        "                                        cache_subdir='models',\n",
        "                                        md5_hash='55e62a6358af8a0af0eedf399b5aea99')\n",
        "            model.load_weights(weights_path)\n",
        "            weights_loaded = True\n",
        "\n",
        "        if (depth == 161) and (nb_dense_block == 4) and (growth_rate == 48) and (nb_filter == 96) and \\\n",
        "                (bottleneck is True) and (reduction == 0.5) and (dropout_rate == 0.0) and (subsample_initial_block):\n",
        "            if include_top:\n",
        "                weights_path = get_file('DenseNet-BC-161-48.h5',\n",
        "                                        DENSENET_161_WEIGHTS_PATH,\n",
        "                                        cache_subdir='models',\n",
        "                                        md5_hash='6c326cf4fbdb57d31eff04333a23fcca')\n",
        "            else:\n",
        "                weights_path = get_file('DenseNet-BC-161-48-no-top.h5',\n",
        "                                        DENSENET_161_WEIGHTS_PATH_NO_TOP,\n",
        "                                        cache_subdir='models',\n",
        "                                        md5_hash='1a9476b79f6b7673acaa2769e6427b92')\n",
        "            model.load_weights(weights_path)\n",
        "            weights_loaded = True\n",
        "\n",
        "        if (depth == 169) and (nb_dense_block == 4) and (growth_rate == 32) and (nb_filter == 64) and \\\n",
        "                (bottleneck is True) and (reduction == 0.5) and (dropout_rate == 0.0) and (subsample_initial_block):\n",
        "            if include_top:\n",
        "                weights_path = get_file('DenseNet-BC-169-32.h5',\n",
        "                                        DENSENET_169_WEIGHTS_PATH,\n",
        "                                        cache_subdir='models',\n",
        "                                        md5_hash='914869c361303d2e39dec640b4e606a6')\n",
        "            else:\n",
        "                weights_path = get_file('DenseNet-BC-169-32-no-top.h5',\n",
        "                                        DENSENET_169_WEIGHTS_PATH_NO_TOP,\n",
        "                                        cache_subdir='models',\n",
        "                                        md5_hash='89c19e8276cfd10585d5fadc1df6859e')\n",
        "            model.load_weights(weights_path)\n",
        "            weights_loaded = True\n",
        "\n",
        "        if weights_loaded:\n",
        "            if K.backend() == 'theano':\n",
        "                convert_all_kernels_in_model(model)\n",
        "\n",
        "            if K.image_data_format() == 'channels_first' and K.backend() == 'tensorflow':\n",
        "                warnings.warn('You are using the TensorFlow backend, yet you '\n",
        "                              'are using the Theano '\n",
        "                              'image data format convention '\n",
        "                              '(`image_data_format=\"channels_first\"`). '\n",
        "                              'For best performance, set '\n",
        "                              '`image_data_format=\"channels_last\"` in '\n",
        "                              'your Keras config '\n",
        "                              'at ~/.keras/keras.json.')\n",
        "\n",
        "            print(\"Weights for the model were loaded successfully\")\"\"\"\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def __conv_block(ip, nb_filter, bottleneck=False, dropout_rate=None, weight_decay=1e-4):\n",
        "    ''' Apply BatchNorm, Relu, 3x3 Conv2D, optional bottleneck block and dropout\n",
        "    Args:\n",
        "        ip: Input keras tensor\n",
        "        nb_filter: number of filters\n",
        "        bottleneck: add bottleneck block\n",
        "        dropout_rate: dropout rate\n",
        "        weight_decay: weight decay factor\n",
        "    Returns: keras tensor with batch_norm, relu and convolution2d added (optional bottleneck)\n",
        "    '''\n",
        "    concat_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
        "\n",
        "    x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(ip)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    if bottleneck:\n",
        "        inter_channel = nb_filter * 4  # Obtained from https://github.com/liuzhuang13/DenseNet/blob/master/densenet.lua\n",
        "\n",
        "        x = Conv2D(inter_channel, (1, 1), kernel_initializer='he_normal', padding='same', use_bias=False,\n",
        "                   kernel_regularizer=l2(weight_decay))(x)\n",
        "        x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(x)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(nb_filter, (3, 3), kernel_initializer='he_normal', padding='same', use_bias=False)(x)\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def __dense_block(x, nb_layers, nb_filter, growth_rate, bottleneck=False, dropout_rate=None, weight_decay=1e-4,\n",
        "                  grow_nb_filters=True, return_concat_list=False):\n",
        "    ''' Build a dense_block where the output of each conv_block is fed to subsequent ones\n",
        "    Args:\n",
        "        x: keras tensor\n",
        "        nb_layers: the number of layers of conv_block to append to the model.\n",
        "        nb_filter: number of filters\n",
        "        growth_rate: growth rate\n",
        "        bottleneck: bottleneck block\n",
        "        dropout_rate: dropout rate\n",
        "        weight_decay: weight decay factor\n",
        "        grow_nb_filters: flag to decide to allow number of filters to grow\n",
        "        return_concat_list: return the list of feature maps along with the actual output\n",
        "    Returns: keras tensor with nb_layers of conv_block appended\n",
        "    '''\n",
        "    concat_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
        "\n",
        "    x_list = [x]\n",
        "\n",
        "    for i in range(nb_layers):\n",
        "        cb = __conv_block(x, growth_rate, bottleneck, dropout_rate, weight_decay)\n",
        "        x_list.append(cb)\n",
        "\n",
        "        x = concatenate([x, cb], axis=concat_axis)\n",
        "\n",
        "        if grow_nb_filters:\n",
        "            nb_filter += growth_rate\n",
        "\n",
        "    if return_concat_list:\n",
        "        return x, nb_filter, x_list\n",
        "    else:\n",
        "        return x, nb_filter\n",
        "\n",
        "\n",
        "def __transition_block(ip, nb_filter, compression=1.0, weight_decay=1e-4):\n",
        "    ''' Apply BatchNorm, Relu 1x1, Conv2D, optional compression, dropout and Maxpooling2D\n",
        "    Args:\n",
        "        ip: keras tensor\n",
        "        nb_filter: number of filters\n",
        "        compression: calculated as 1 - reduction. Reduces the number of feature maps\n",
        "                    in the transition block.\n",
        "        dropout_rate: dropout rate\n",
        "        weight_decay: weight decay factor\n",
        "    Returns: keras tensor, after applying batch_norm, relu-conv, dropout, maxpool\n",
        "    '''\n",
        "    concat_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
        "\n",
        "    x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(ip)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(int(nb_filter * compression), (1, 1), kernel_initializer='he_normal', padding='same', use_bias=False,\n",
        "               kernel_regularizer=l2(weight_decay))(x)\n",
        "    x = AveragePooling2D((2, 2), strides=(2, 2))(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def __transition_up_block(ip, nb_filters, type='deconv', weight_decay=1E-4):\n",
        "    ''' SubpixelConvolutional Upscaling (factor = 2)\n",
        "    Args:\n",
        "        ip: keras tensor\n",
        "        nb_filters: number of layers\n",
        "        type: can be 'upsampling', 'subpixel', 'deconv'. Determines type of upsampling performed\n",
        "        weight_decay: weight decay factor\n",
        "    Returns: keras tensor, after applying upsampling operation.\n",
        "    '''\n",
        "\n",
        "    if type == 'upsampling':\n",
        "        x = UpSampling2D()(ip)\n",
        "    elif type == 'subpixel':\n",
        "        x = Conv2D(nb_filters, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(weight_decay),\n",
        "                   use_bias=False, kernel_initializer='he_normal')(ip)\n",
        "        x = SubPixelUpscaling(scale_factor=2)(x)\n",
        "        x = Conv2D(nb_filters, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(weight_decay),\n",
        "                   use_bias=False, kernel_initializer='he_normal')(x)\n",
        "    else:\n",
        "        x = Conv2DTranspose(nb_filters, (3, 3), activation='relu', padding='same', strides=(2, 2),\n",
        "                            kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay))(ip)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def __create_dense_net(nb_classes, img_input, include_top, depth=40, nb_dense_block=3, growth_rate=12, nb_filter=-1,\n",
        "                       nb_layers_per_block=-1, bottleneck=False, reduction=0.0, dropout_rate=None, weight_decay=1e-4,\n",
        "                       subsample_initial_block=False, activation='softmax'):\n",
        "    ''' Build the DenseNet model\n",
        "    Args:\n",
        "        nb_classes: number of classes\n",
        "        img_input: tuple of shape (channels, rows, columns) or (rows, columns, channels)\n",
        "        include_top: flag to include the final Dense layer\n",
        "        depth: number or layers\n",
        "        nb_dense_block: number of dense blocks to add to end (generally = 3)\n",
        "        growth_rate: number of filters to add per dense block\n",
        "        nb_filter: initial number of filters. Default -1 indicates initial number of filters is 2 * growth_rate\n",
        "        nb_layers_per_block: number of layers in each dense block.\n",
        "                Can be a -1, positive integer or a list.\n",
        "                If -1, calculates nb_layer_per_block from the depth of the network.\n",
        "                If positive integer, a set number of layers per dense block.\n",
        "                If list, nb_layer is used as provided. Note that list size must\n",
        "                be (nb_dense_block + 1)\n",
        "        bottleneck: add bottleneck blocks\n",
        "        reduction: reduction factor of transition blocks. Note : reduction value is inverted to compute compression\n",
        "        dropout_rate: dropout rate\n",
        "        weight_decay: weight decay rate\n",
        "        subsample_initial_block: Set to True to subsample the initial convolution and\n",
        "                add a MaxPool2D before the dense blocks are added.\n",
        "        subsample_initial:\n",
        "        activation: Type of activation at the top layer. Can be one of 'softmax' or 'sigmoid'.\n",
        "                Note that if sigmoid is used, classes must be 1.\n",
        "    Returns: keras tensor with nb_layers of conv_block appended\n",
        "    '''\n",
        "\n",
        "    concat_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
        "\n",
        "    if reduction != 0.0:\n",
        "        assert reduction <= 1.0 and reduction > 0.0, 'reduction value must lie between 0.0 and 1.0'\n",
        "\n",
        "    # layers in each dense block\n",
        "    if type(nb_layers_per_block) is list or type(nb_layers_per_block) is tuple:\n",
        "        nb_layers = list(nb_layers_per_block)  # Convert tuple to list\n",
        "\n",
        "        assert len(nb_layers) == (nb_dense_block), 'If list, nb_layer is used as provided. ' \\\n",
        "                                                   'Note that list size must be (nb_dense_block)'\n",
        "        final_nb_layer = nb_layers[-1]\n",
        "        nb_layers = nb_layers[:-1]\n",
        "    else:\n",
        "        if nb_layers_per_block == -1:\n",
        "            assert (depth - 4) % 3 == 0, 'Depth must be 3 N + 4 if nb_layers_per_block == -1'\n",
        "            count = int((depth - 4) / 3)\n",
        "\n",
        "            if bottleneck:\n",
        "                count = count // 2\n",
        "\n",
        "            nb_layers = [count for _ in range(nb_dense_block)]\n",
        "            final_nb_layer = count\n",
        "        else:\n",
        "            final_nb_layer = nb_layers_per_block\n",
        "            nb_layers = [nb_layers_per_block] * nb_dense_block\n",
        "\n",
        "    # compute initial nb_filter if -1, else accept users initial nb_filter\n",
        "    if nb_filter <= 0:\n",
        "        nb_filter = 2 * growth_rate\n",
        "\n",
        "    # compute compression factor\n",
        "    compression = 1.0 - reduction\n",
        "\n",
        "    # Initial convolution\n",
        "    if subsample_initial_block:\n",
        "        initial_kernel = (7, 7)\n",
        "        initial_strides = (2, 2)\n",
        "    else:\n",
        "        initial_kernel = (3, 3)\n",
        "        initial_strides = (1, 1)\n",
        "\n",
        "    x = Conv2D(nb_filter, initial_kernel, kernel_initializer='he_normal', padding='same',\n",
        "               strides=initial_strides, use_bias=False, kernel_regularizer=l2(weight_decay))(img_input)\n",
        "\n",
        "    if subsample_initial_block:\n",
        "        x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(x)\n",
        "        x = Activation('relu')(x)\n",
        "        x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "\n",
        "    # Add dense blocks\n",
        "    for block_idx in range(nb_dense_block - 1):\n",
        "        x, nb_filter = __dense_block(x, nb_layers[block_idx], nb_filter, growth_rate, bottleneck=bottleneck,\n",
        "                                     dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
        "        # add transition_block\n",
        "        x = __transition_block(x, nb_filter, compression=compression, weight_decay=weight_decay)\n",
        "        nb_filter = int(nb_filter * compression)\n",
        "\n",
        "    # The last dense_block does not have a transition_block\n",
        "    x, nb_filter = __dense_block(x, final_nb_layer, nb_filter, growth_rate, bottleneck=bottleneck,\n",
        "                                 dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
        "\n",
        "    x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "    if include_top:\n",
        "        x = Dense(nb_classes, activation=activation)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#!pip install pydot\n",
        "#import pydot\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    from keras.utils.vis_utils import plot_model\n",
        "    #model = DenseNetFCN((32, 32, 3), growth_rate=16, nb_layers_per_block=[4, 5, 7, 10, 12, 15], upsampling_type='deconv')\n",
        "    model = DenseNet((32, 32, 3), depth=100, nb_dense_block=3,\n",
        "                     growth_rate=12, bottleneck=True, reduction=0.5, weights=None)\n",
        "    model.summary()\n",
        "\n",
        "    from keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "#    plot_model(model, 'test.png', show_shapes=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_298 (Conv2D)             (None, 32, 32, 24)   648         input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_298 (BatchN (None, 32, 32, 24)   96          conv2d_298[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_298 (Activation)     (None, 32, 32, 24)   0           batch_normalization_298[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_299 (Conv2D)             (None, 32, 32, 48)   1152        activation_298[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_299 (BatchN (None, 32, 32, 48)   192         conv2d_299[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_299 (Activation)     (None, 32, 32, 48)   0           batch_normalization_299[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_300 (Conv2D)             (None, 32, 32, 12)   5184        activation_299[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_145 (Concatenate)   (None, 32, 32, 36)   0           conv2d_298[0][0]                 \n",
            "                                                                 conv2d_300[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_300 (BatchN (None, 32, 32, 36)   144         concatenate_145[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_300 (Activation)     (None, 32, 32, 36)   0           batch_normalization_300[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_301 (Conv2D)             (None, 32, 32, 48)   1728        activation_300[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_301 (BatchN (None, 32, 32, 48)   192         conv2d_301[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_301 (Activation)     (None, 32, 32, 48)   0           batch_normalization_301[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_302 (Conv2D)             (None, 32, 32, 12)   5184        activation_301[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_146 (Concatenate)   (None, 32, 32, 48)   0           concatenate_145[0][0]            \n",
            "                                                                 conv2d_302[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_302 (BatchN (None, 32, 32, 48)   192         concatenate_146[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_302 (Activation)     (None, 32, 32, 48)   0           batch_normalization_302[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_303 (Conv2D)             (None, 32, 32, 48)   2304        activation_302[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_303 (BatchN (None, 32, 32, 48)   192         conv2d_303[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_303 (Activation)     (None, 32, 32, 48)   0           batch_normalization_303[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_304 (Conv2D)             (None, 32, 32, 12)   5184        activation_303[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_147 (Concatenate)   (None, 32, 32, 60)   0           concatenate_146[0][0]            \n",
            "                                                                 conv2d_304[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_304 (BatchN (None, 32, 32, 60)   240         concatenate_147[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_304 (Activation)     (None, 32, 32, 60)   0           batch_normalization_304[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_305 (Conv2D)             (None, 32, 32, 48)   2880        activation_304[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_305 (BatchN (None, 32, 32, 48)   192         conv2d_305[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_305 (Activation)     (None, 32, 32, 48)   0           batch_normalization_305[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_306 (Conv2D)             (None, 32, 32, 12)   5184        activation_305[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_148 (Concatenate)   (None, 32, 32, 72)   0           concatenate_147[0][0]            \n",
            "                                                                 conv2d_306[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_306 (BatchN (None, 32, 32, 72)   288         concatenate_148[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_306 (Activation)     (None, 32, 32, 72)   0           batch_normalization_306[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_307 (Conv2D)             (None, 32, 32, 48)   3456        activation_306[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_307 (BatchN (None, 32, 32, 48)   192         conv2d_307[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_307 (Activation)     (None, 32, 32, 48)   0           batch_normalization_307[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_308 (Conv2D)             (None, 32, 32, 12)   5184        activation_307[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_149 (Concatenate)   (None, 32, 32, 84)   0           concatenate_148[0][0]            \n",
            "                                                                 conv2d_308[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_308 (BatchN (None, 32, 32, 84)   336         concatenate_149[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_308 (Activation)     (None, 32, 32, 84)   0           batch_normalization_308[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_309 (Conv2D)             (None, 32, 32, 48)   4032        activation_308[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_309 (BatchN (None, 32, 32, 48)   192         conv2d_309[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_309 (Activation)     (None, 32, 32, 48)   0           batch_normalization_309[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_310 (Conv2D)             (None, 32, 32, 12)   5184        activation_309[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_150 (Concatenate)   (None, 32, 32, 96)   0           concatenate_149[0][0]            \n",
            "                                                                 conv2d_310[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_310 (BatchN (None, 32, 32, 96)   384         concatenate_150[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_310 (Activation)     (None, 32, 32, 96)   0           batch_normalization_310[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_311 (Conv2D)             (None, 32, 32, 48)   4608        activation_310[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_311 (BatchN (None, 32, 32, 48)   192         conv2d_311[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_311 (Activation)     (None, 32, 32, 48)   0           batch_normalization_311[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_312 (Conv2D)             (None, 32, 32, 12)   5184        activation_311[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_151 (Concatenate)   (None, 32, 32, 108)  0           concatenate_150[0][0]            \n",
            "                                                                 conv2d_312[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_312 (BatchN (None, 32, 32, 108)  432         concatenate_151[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_312 (Activation)     (None, 32, 32, 108)  0           batch_normalization_312[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_313 (Conv2D)             (None, 32, 32, 48)   5184        activation_312[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_313 (BatchN (None, 32, 32, 48)   192         conv2d_313[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_313 (Activation)     (None, 32, 32, 48)   0           batch_normalization_313[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_314 (Conv2D)             (None, 32, 32, 12)   5184        activation_313[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_152 (Concatenate)   (None, 32, 32, 120)  0           concatenate_151[0][0]            \n",
            "                                                                 conv2d_314[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_314 (BatchN (None, 32, 32, 120)  480         concatenate_152[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_314 (Activation)     (None, 32, 32, 120)  0           batch_normalization_314[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_315 (Conv2D)             (None, 32, 32, 48)   5760        activation_314[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_315 (BatchN (None, 32, 32, 48)   192         conv2d_315[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_315 (Activation)     (None, 32, 32, 48)   0           batch_normalization_315[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_316 (Conv2D)             (None, 32, 32, 12)   5184        activation_315[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_153 (Concatenate)   (None, 32, 32, 132)  0           concatenate_152[0][0]            \n",
            "                                                                 conv2d_316[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_316 (BatchN (None, 32, 32, 132)  528         concatenate_153[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_316 (Activation)     (None, 32, 32, 132)  0           batch_normalization_316[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_317 (Conv2D)             (None, 32, 32, 48)   6336        activation_316[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_317 (BatchN (None, 32, 32, 48)   192         conv2d_317[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_317 (Activation)     (None, 32, 32, 48)   0           batch_normalization_317[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_318 (Conv2D)             (None, 32, 32, 12)   5184        activation_317[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_154 (Concatenate)   (None, 32, 32, 144)  0           concatenate_153[0][0]            \n",
            "                                                                 conv2d_318[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_318 (BatchN (None, 32, 32, 144)  576         concatenate_154[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_318 (Activation)     (None, 32, 32, 144)  0           batch_normalization_318[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_319 (Conv2D)             (None, 32, 32, 48)   6912        activation_318[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_319 (BatchN (None, 32, 32, 48)   192         conv2d_319[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_319 (Activation)     (None, 32, 32, 48)   0           batch_normalization_319[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_320 (Conv2D)             (None, 32, 32, 12)   5184        activation_319[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_155 (Concatenate)   (None, 32, 32, 156)  0           concatenate_154[0][0]            \n",
            "                                                                 conv2d_320[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_320 (BatchN (None, 32, 32, 156)  624         concatenate_155[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_320 (Activation)     (None, 32, 32, 156)  0           batch_normalization_320[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_321 (Conv2D)             (None, 32, 32, 48)   7488        activation_320[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_321 (BatchN (None, 32, 32, 48)   192         conv2d_321[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_321 (Activation)     (None, 32, 32, 48)   0           batch_normalization_321[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_322 (Conv2D)             (None, 32, 32, 12)   5184        activation_321[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_156 (Concatenate)   (None, 32, 32, 168)  0           concatenate_155[0][0]            \n",
            "                                                                 conv2d_322[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_322 (BatchN (None, 32, 32, 168)  672         concatenate_156[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_322 (Activation)     (None, 32, 32, 168)  0           batch_normalization_322[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_323 (Conv2D)             (None, 32, 32, 48)   8064        activation_322[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_323 (BatchN (None, 32, 32, 48)   192         conv2d_323[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_323 (Activation)     (None, 32, 32, 48)   0           batch_normalization_323[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_324 (Conv2D)             (None, 32, 32, 12)   5184        activation_323[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_157 (Concatenate)   (None, 32, 32, 180)  0           concatenate_156[0][0]            \n",
            "                                                                 conv2d_324[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_324 (BatchN (None, 32, 32, 180)  720         concatenate_157[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_324 (Activation)     (None, 32, 32, 180)  0           batch_normalization_324[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_325 (Conv2D)             (None, 32, 32, 48)   8640        activation_324[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_325 (BatchN (None, 32, 32, 48)   192         conv2d_325[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_325 (Activation)     (None, 32, 32, 48)   0           batch_normalization_325[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_326 (Conv2D)             (None, 32, 32, 12)   5184        activation_325[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_158 (Concatenate)   (None, 32, 32, 192)  0           concatenate_157[0][0]            \n",
            "                                                                 conv2d_326[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_326 (BatchN (None, 32, 32, 192)  768         concatenate_158[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_326 (Activation)     (None, 32, 32, 192)  0           batch_normalization_326[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_327 (Conv2D)             (None, 32, 32, 48)   9216        activation_326[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_327 (BatchN (None, 32, 32, 48)   192         conv2d_327[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_327 (Activation)     (None, 32, 32, 48)   0           batch_normalization_327[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_328 (Conv2D)             (None, 32, 32, 12)   5184        activation_327[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_159 (Concatenate)   (None, 32, 32, 204)  0           concatenate_158[0][0]            \n",
            "                                                                 conv2d_328[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_328 (BatchN (None, 32, 32, 204)  816         concatenate_159[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_328 (Activation)     (None, 32, 32, 204)  0           batch_normalization_328[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_329 (Conv2D)             (None, 32, 32, 48)   9792        activation_328[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_329 (BatchN (None, 32, 32, 48)   192         conv2d_329[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_329 (Activation)     (None, 32, 32, 48)   0           batch_normalization_329[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_330 (Conv2D)             (None, 32, 32, 12)   5184        activation_329[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_160 (Concatenate)   (None, 32, 32, 216)  0           concatenate_159[0][0]            \n",
            "                                                                 conv2d_330[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_330 (BatchN (None, 32, 32, 216)  864         concatenate_160[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_330 (Activation)     (None, 32, 32, 216)  0           batch_normalization_330[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_331 (Conv2D)             (None, 32, 32, 108)  23328       activation_330[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_7 (AveragePoo (None, 16, 16, 108)  0           conv2d_331[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_331 (BatchN (None, 16, 16, 108)  432         average_pooling2d_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_331 (Activation)     (None, 16, 16, 108)  0           batch_normalization_331[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_332 (Conv2D)             (None, 16, 16, 48)   5184        activation_331[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_332 (BatchN (None, 16, 16, 48)   192         conv2d_332[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_332 (Activation)     (None, 16, 16, 48)   0           batch_normalization_332[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_333 (Conv2D)             (None, 16, 16, 12)   5184        activation_332[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_161 (Concatenate)   (None, 16, 16, 120)  0           average_pooling2d_7[0][0]        \n",
            "                                                                 conv2d_333[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_333 (BatchN (None, 16, 16, 120)  480         concatenate_161[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_333 (Activation)     (None, 16, 16, 120)  0           batch_normalization_333[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_334 (Conv2D)             (None, 16, 16, 48)   5760        activation_333[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_334 (BatchN (None, 16, 16, 48)   192         conv2d_334[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_334 (Activation)     (None, 16, 16, 48)   0           batch_normalization_334[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_335 (Conv2D)             (None, 16, 16, 12)   5184        activation_334[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_162 (Concatenate)   (None, 16, 16, 132)  0           concatenate_161[0][0]            \n",
            "                                                                 conv2d_335[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_335 (BatchN (None, 16, 16, 132)  528         concatenate_162[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_335 (Activation)     (None, 16, 16, 132)  0           batch_normalization_335[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_336 (Conv2D)             (None, 16, 16, 48)   6336        activation_335[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_336 (BatchN (None, 16, 16, 48)   192         conv2d_336[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_336 (Activation)     (None, 16, 16, 48)   0           batch_normalization_336[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_337 (Conv2D)             (None, 16, 16, 12)   5184        activation_336[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_163 (Concatenate)   (None, 16, 16, 144)  0           concatenate_162[0][0]            \n",
            "                                                                 conv2d_337[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_337 (BatchN (None, 16, 16, 144)  576         concatenate_163[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_337 (Activation)     (None, 16, 16, 144)  0           batch_normalization_337[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_338 (Conv2D)             (None, 16, 16, 48)   6912        activation_337[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_338 (BatchN (None, 16, 16, 48)   192         conv2d_338[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_338 (Activation)     (None, 16, 16, 48)   0           batch_normalization_338[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_339 (Conv2D)             (None, 16, 16, 12)   5184        activation_338[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_164 (Concatenate)   (None, 16, 16, 156)  0           concatenate_163[0][0]            \n",
            "                                                                 conv2d_339[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_339 (BatchN (None, 16, 16, 156)  624         concatenate_164[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_339 (Activation)     (None, 16, 16, 156)  0           batch_normalization_339[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_340 (Conv2D)             (None, 16, 16, 48)   7488        activation_339[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_340 (BatchN (None, 16, 16, 48)   192         conv2d_340[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_340 (Activation)     (None, 16, 16, 48)   0           batch_normalization_340[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_341 (Conv2D)             (None, 16, 16, 12)   5184        activation_340[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_165 (Concatenate)   (None, 16, 16, 168)  0           concatenate_164[0][0]            \n",
            "                                                                 conv2d_341[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_341 (BatchN (None, 16, 16, 168)  672         concatenate_165[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_341 (Activation)     (None, 16, 16, 168)  0           batch_normalization_341[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_342 (Conv2D)             (None, 16, 16, 48)   8064        activation_341[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_342 (BatchN (None, 16, 16, 48)   192         conv2d_342[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_342 (Activation)     (None, 16, 16, 48)   0           batch_normalization_342[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_343 (Conv2D)             (None, 16, 16, 12)   5184        activation_342[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_166 (Concatenate)   (None, 16, 16, 180)  0           concatenate_165[0][0]            \n",
            "                                                                 conv2d_343[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_343 (BatchN (None, 16, 16, 180)  720         concatenate_166[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_343 (Activation)     (None, 16, 16, 180)  0           batch_normalization_343[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_344 (Conv2D)             (None, 16, 16, 48)   8640        activation_343[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_344 (BatchN (None, 16, 16, 48)   192         conv2d_344[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_344 (Activation)     (None, 16, 16, 48)   0           batch_normalization_344[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_345 (Conv2D)             (None, 16, 16, 12)   5184        activation_344[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_167 (Concatenate)   (None, 16, 16, 192)  0           concatenate_166[0][0]            \n",
            "                                                                 conv2d_345[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_345 (BatchN (None, 16, 16, 192)  768         concatenate_167[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_345 (Activation)     (None, 16, 16, 192)  0           batch_normalization_345[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_346 (Conv2D)             (None, 16, 16, 48)   9216        activation_345[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_346 (BatchN (None, 16, 16, 48)   192         conv2d_346[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_346 (Activation)     (None, 16, 16, 48)   0           batch_normalization_346[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_347 (Conv2D)             (None, 16, 16, 12)   5184        activation_346[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_168 (Concatenate)   (None, 16, 16, 204)  0           concatenate_167[0][0]            \n",
            "                                                                 conv2d_347[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_347 (BatchN (None, 16, 16, 204)  816         concatenate_168[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_347 (Activation)     (None, 16, 16, 204)  0           batch_normalization_347[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_348 (Conv2D)             (None, 16, 16, 48)   9792        activation_347[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_348 (BatchN (None, 16, 16, 48)   192         conv2d_348[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_348 (Activation)     (None, 16, 16, 48)   0           batch_normalization_348[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_349 (Conv2D)             (None, 16, 16, 12)   5184        activation_348[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_169 (Concatenate)   (None, 16, 16, 216)  0           concatenate_168[0][0]            \n",
            "                                                                 conv2d_349[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_349 (BatchN (None, 16, 16, 216)  864         concatenate_169[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_349 (Activation)     (None, 16, 16, 216)  0           batch_normalization_349[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_350 (Conv2D)             (None, 16, 16, 48)   10368       activation_349[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_350 (BatchN (None, 16, 16, 48)   192         conv2d_350[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_350 (Activation)     (None, 16, 16, 48)   0           batch_normalization_350[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_351 (Conv2D)             (None, 16, 16, 12)   5184        activation_350[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_170 (Concatenate)   (None, 16, 16, 228)  0           concatenate_169[0][0]            \n",
            "                                                                 conv2d_351[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_351 (BatchN (None, 16, 16, 228)  912         concatenate_170[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_351 (Activation)     (None, 16, 16, 228)  0           batch_normalization_351[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_352 (Conv2D)             (None, 16, 16, 48)   10944       activation_351[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_352 (BatchN (None, 16, 16, 48)   192         conv2d_352[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_352 (Activation)     (None, 16, 16, 48)   0           batch_normalization_352[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_353 (Conv2D)             (None, 16, 16, 12)   5184        activation_352[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_171 (Concatenate)   (None, 16, 16, 240)  0           concatenate_170[0][0]            \n",
            "                                                                 conv2d_353[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_353 (BatchN (None, 16, 16, 240)  960         concatenate_171[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_353 (Activation)     (None, 16, 16, 240)  0           batch_normalization_353[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_354 (Conv2D)             (None, 16, 16, 48)   11520       activation_353[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_354 (BatchN (None, 16, 16, 48)   192         conv2d_354[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_354 (Activation)     (None, 16, 16, 48)   0           batch_normalization_354[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_355 (Conv2D)             (None, 16, 16, 12)   5184        activation_354[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_172 (Concatenate)   (None, 16, 16, 252)  0           concatenate_171[0][0]            \n",
            "                                                                 conv2d_355[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_355 (BatchN (None, 16, 16, 252)  1008        concatenate_172[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_355 (Activation)     (None, 16, 16, 252)  0           batch_normalization_355[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_356 (Conv2D)             (None, 16, 16, 48)   12096       activation_355[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_356 (BatchN (None, 16, 16, 48)   192         conv2d_356[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_356 (Activation)     (None, 16, 16, 48)   0           batch_normalization_356[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_357 (Conv2D)             (None, 16, 16, 12)   5184        activation_356[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_173 (Concatenate)   (None, 16, 16, 264)  0           concatenate_172[0][0]            \n",
            "                                                                 conv2d_357[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_357 (BatchN (None, 16, 16, 264)  1056        concatenate_173[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_357 (Activation)     (None, 16, 16, 264)  0           batch_normalization_357[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_358 (Conv2D)             (None, 16, 16, 48)   12672       activation_357[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_358 (BatchN (None, 16, 16, 48)   192         conv2d_358[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_358 (Activation)     (None, 16, 16, 48)   0           batch_normalization_358[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_359 (Conv2D)             (None, 16, 16, 12)   5184        activation_358[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_174 (Concatenate)   (None, 16, 16, 276)  0           concatenate_173[0][0]            \n",
            "                                                                 conv2d_359[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_359 (BatchN (None, 16, 16, 276)  1104        concatenate_174[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_359 (Activation)     (None, 16, 16, 276)  0           batch_normalization_359[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_360 (Conv2D)             (None, 16, 16, 48)   13248       activation_359[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_360 (BatchN (None, 16, 16, 48)   192         conv2d_360[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_360 (Activation)     (None, 16, 16, 48)   0           batch_normalization_360[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_361 (Conv2D)             (None, 16, 16, 12)   5184        activation_360[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_175 (Concatenate)   (None, 16, 16, 288)  0           concatenate_174[0][0]            \n",
            "                                                                 conv2d_361[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_361 (BatchN (None, 16, 16, 288)  1152        concatenate_175[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_361 (Activation)     (None, 16, 16, 288)  0           batch_normalization_361[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_362 (Conv2D)             (None, 16, 16, 48)   13824       activation_361[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_362 (BatchN (None, 16, 16, 48)   192         conv2d_362[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_362 (Activation)     (None, 16, 16, 48)   0           batch_normalization_362[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_363 (Conv2D)             (None, 16, 16, 12)   5184        activation_362[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_176 (Concatenate)   (None, 16, 16, 300)  0           concatenate_175[0][0]            \n",
            "                                                                 conv2d_363[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_363 (BatchN (None, 16, 16, 300)  1200        concatenate_176[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_363 (Activation)     (None, 16, 16, 300)  0           batch_normalization_363[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_364 (Conv2D)             (None, 16, 16, 150)  45000       activation_363[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_8 (AveragePoo (None, 8, 8, 150)    0           conv2d_364[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_364 (BatchN (None, 8, 8, 150)    600         average_pooling2d_8[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_364 (Activation)     (None, 8, 8, 150)    0           batch_normalization_364[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_365 (Conv2D)             (None, 8, 8, 48)     7200        activation_364[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_365 (BatchN (None, 8, 8, 48)     192         conv2d_365[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_365 (Activation)     (None, 8, 8, 48)     0           batch_normalization_365[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_366 (Conv2D)             (None, 8, 8, 12)     5184        activation_365[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_177 (Concatenate)   (None, 8, 8, 162)    0           average_pooling2d_8[0][0]        \n",
            "                                                                 conv2d_366[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_366 (BatchN (None, 8, 8, 162)    648         concatenate_177[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_366 (Activation)     (None, 8, 8, 162)    0           batch_normalization_366[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_367 (Conv2D)             (None, 8, 8, 48)     7776        activation_366[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_367 (BatchN (None, 8, 8, 48)     192         conv2d_367[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_367 (Activation)     (None, 8, 8, 48)     0           batch_normalization_367[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_368 (Conv2D)             (None, 8, 8, 12)     5184        activation_367[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_178 (Concatenate)   (None, 8, 8, 174)    0           concatenate_177[0][0]            \n",
            "                                                                 conv2d_368[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_368 (BatchN (None, 8, 8, 174)    696         concatenate_178[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_368 (Activation)     (None, 8, 8, 174)    0           batch_normalization_368[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_369 (Conv2D)             (None, 8, 8, 48)     8352        activation_368[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_369 (BatchN (None, 8, 8, 48)     192         conv2d_369[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_369 (Activation)     (None, 8, 8, 48)     0           batch_normalization_369[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_370 (Conv2D)             (None, 8, 8, 12)     5184        activation_369[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_179 (Concatenate)   (None, 8, 8, 186)    0           concatenate_178[0][0]            \n",
            "                                                                 conv2d_370[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_370 (BatchN (None, 8, 8, 186)    744         concatenate_179[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_370 (Activation)     (None, 8, 8, 186)    0           batch_normalization_370[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_371 (Conv2D)             (None, 8, 8, 48)     8928        activation_370[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_371 (BatchN (None, 8, 8, 48)     192         conv2d_371[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_371 (Activation)     (None, 8, 8, 48)     0           batch_normalization_371[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_372 (Conv2D)             (None, 8, 8, 12)     5184        activation_371[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_180 (Concatenate)   (None, 8, 8, 198)    0           concatenate_179[0][0]            \n",
            "                                                                 conv2d_372[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_372 (BatchN (None, 8, 8, 198)    792         concatenate_180[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_372 (Activation)     (None, 8, 8, 198)    0           batch_normalization_372[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_373 (Conv2D)             (None, 8, 8, 48)     9504        activation_372[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_373 (BatchN (None, 8, 8, 48)     192         conv2d_373[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_373 (Activation)     (None, 8, 8, 48)     0           batch_normalization_373[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_374 (Conv2D)             (None, 8, 8, 12)     5184        activation_373[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_181 (Concatenate)   (None, 8, 8, 210)    0           concatenate_180[0][0]            \n",
            "                                                                 conv2d_374[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_374 (BatchN (None, 8, 8, 210)    840         concatenate_181[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_374 (Activation)     (None, 8, 8, 210)    0           batch_normalization_374[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_375 (Conv2D)             (None, 8, 8, 48)     10080       activation_374[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_375 (BatchN (None, 8, 8, 48)     192         conv2d_375[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_375 (Activation)     (None, 8, 8, 48)     0           batch_normalization_375[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_376 (Conv2D)             (None, 8, 8, 12)     5184        activation_375[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_182 (Concatenate)   (None, 8, 8, 222)    0           concatenate_181[0][0]            \n",
            "                                                                 conv2d_376[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_376 (BatchN (None, 8, 8, 222)    888         concatenate_182[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_376 (Activation)     (None, 8, 8, 222)    0           batch_normalization_376[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_377 (Conv2D)             (None, 8, 8, 48)     10656       activation_376[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_377 (BatchN (None, 8, 8, 48)     192         conv2d_377[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_377 (Activation)     (None, 8, 8, 48)     0           batch_normalization_377[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_378 (Conv2D)             (None, 8, 8, 12)     5184        activation_377[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_183 (Concatenate)   (None, 8, 8, 234)    0           concatenate_182[0][0]            \n",
            "                                                                 conv2d_378[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_378 (BatchN (None, 8, 8, 234)    936         concatenate_183[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_378 (Activation)     (None, 8, 8, 234)    0           batch_normalization_378[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_379 (Conv2D)             (None, 8, 8, 48)     11232       activation_378[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_379 (BatchN (None, 8, 8, 48)     192         conv2d_379[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_379 (Activation)     (None, 8, 8, 48)     0           batch_normalization_379[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_380 (Conv2D)             (None, 8, 8, 12)     5184        activation_379[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_184 (Concatenate)   (None, 8, 8, 246)    0           concatenate_183[0][0]            \n",
            "                                                                 conv2d_380[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_380 (BatchN (None, 8, 8, 246)    984         concatenate_184[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_380 (Activation)     (None, 8, 8, 246)    0           batch_normalization_380[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_381 (Conv2D)             (None, 8, 8, 48)     11808       activation_380[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_381 (BatchN (None, 8, 8, 48)     192         conv2d_381[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_381 (Activation)     (None, 8, 8, 48)     0           batch_normalization_381[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_382 (Conv2D)             (None, 8, 8, 12)     5184        activation_381[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_185 (Concatenate)   (None, 8, 8, 258)    0           concatenate_184[0][0]            \n",
            "                                                                 conv2d_382[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_382 (BatchN (None, 8, 8, 258)    1032        concatenate_185[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_382 (Activation)     (None, 8, 8, 258)    0           batch_normalization_382[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_383 (Conv2D)             (None, 8, 8, 48)     12384       activation_382[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_383 (BatchN (None, 8, 8, 48)     192         conv2d_383[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_383 (Activation)     (None, 8, 8, 48)     0           batch_normalization_383[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_384 (Conv2D)             (None, 8, 8, 12)     5184        activation_383[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_186 (Concatenate)   (None, 8, 8, 270)    0           concatenate_185[0][0]            \n",
            "                                                                 conv2d_384[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_384 (BatchN (None, 8, 8, 270)    1080        concatenate_186[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_384 (Activation)     (None, 8, 8, 270)    0           batch_normalization_384[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_385 (Conv2D)             (None, 8, 8, 48)     12960       activation_384[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_385 (BatchN (None, 8, 8, 48)     192         conv2d_385[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_385 (Activation)     (None, 8, 8, 48)     0           batch_normalization_385[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_386 (Conv2D)             (None, 8, 8, 12)     5184        activation_385[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_187 (Concatenate)   (None, 8, 8, 282)    0           concatenate_186[0][0]            \n",
            "                                                                 conv2d_386[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_386 (BatchN (None, 8, 8, 282)    1128        concatenate_187[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_386 (Activation)     (None, 8, 8, 282)    0           batch_normalization_386[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_387 (Conv2D)             (None, 8, 8, 48)     13536       activation_386[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_387 (BatchN (None, 8, 8, 48)     192         conv2d_387[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_387 (Activation)     (None, 8, 8, 48)     0           batch_normalization_387[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_388 (Conv2D)             (None, 8, 8, 12)     5184        activation_387[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_188 (Concatenate)   (None, 8, 8, 294)    0           concatenate_187[0][0]            \n",
            "                                                                 conv2d_388[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_388 (BatchN (None, 8, 8, 294)    1176        concatenate_188[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_388 (Activation)     (None, 8, 8, 294)    0           batch_normalization_388[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_389 (Conv2D)             (None, 8, 8, 48)     14112       activation_388[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_389 (BatchN (None, 8, 8, 48)     192         conv2d_389[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_389 (Activation)     (None, 8, 8, 48)     0           batch_normalization_389[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_390 (Conv2D)             (None, 8, 8, 12)     5184        activation_389[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_189 (Concatenate)   (None, 8, 8, 306)    0           concatenate_188[0][0]            \n",
            "                                                                 conv2d_390[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_390 (BatchN (None, 8, 8, 306)    1224        concatenate_189[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_390 (Activation)     (None, 8, 8, 306)    0           batch_normalization_390[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_391 (Conv2D)             (None, 8, 8, 48)     14688       activation_390[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_391 (BatchN (None, 8, 8, 48)     192         conv2d_391[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_391 (Activation)     (None, 8, 8, 48)     0           batch_normalization_391[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_392 (Conv2D)             (None, 8, 8, 12)     5184        activation_391[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_190 (Concatenate)   (None, 8, 8, 318)    0           concatenate_189[0][0]            \n",
            "                                                                 conv2d_392[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_392 (BatchN (None, 8, 8, 318)    1272        concatenate_190[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_392 (Activation)     (None, 8, 8, 318)    0           batch_normalization_392[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_393 (Conv2D)             (None, 8, 8, 48)     15264       activation_392[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_393 (BatchN (None, 8, 8, 48)     192         conv2d_393[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_393 (Activation)     (None, 8, 8, 48)     0           batch_normalization_393[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_394 (Conv2D)             (None, 8, 8, 12)     5184        activation_393[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_191 (Concatenate)   (None, 8, 8, 330)    0           concatenate_190[0][0]            \n",
            "                                                                 conv2d_394[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_394 (BatchN (None, 8, 8, 330)    1320        concatenate_191[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_394 (Activation)     (None, 8, 8, 330)    0           batch_normalization_394[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_395 (Conv2D)             (None, 8, 8, 48)     15840       activation_394[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_395 (BatchN (None, 8, 8, 48)     192         conv2d_395[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_395 (Activation)     (None, 8, 8, 48)     0           batch_normalization_395[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_396 (Conv2D)             (None, 8, 8, 12)     5184        activation_395[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_192 (Concatenate)   (None, 8, 8, 342)    0           concatenate_191[0][0]            \n",
            "                                                                 conv2d_396[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_396 (BatchN (None, 8, 8, 342)    1368        concatenate_192[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_396 (Activation)     (None, 8, 8, 342)    0           batch_normalization_396[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_4 (Glo (None, 342)          0           activation_396[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 10)           3430        global_average_pooling2d_4[0][0] \n",
            "==================================================================================================\n",
            "Total params: 793,150\n",
            "Trainable params: 769,162\n",
            "Non-trainable params: 23,988\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ql-mjTXefvjz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import os.path\n",
        "\n",
        "#import densenet\n",
        "import numpy as np\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "avYB8cjDf0yg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "nb_classes = 10\n",
        "nb_epoch = 300\n",
        "\n",
        "img_rows, img_cols = 32, 32\n",
        "img_channels = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZDTu3JTvf4BE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "img_dim = (img_channels, img_rows, img_cols) if K.image_dim_ordering() == \"th\" else (img_rows, img_cols, img_channels)\n",
        "depth = 40\n",
        "nb_dense_block = 3\n",
        "growth_rate = 12\n",
        "nb_filter = 1\n",
        "dropout_rate = 0.0 # 0.0 for data augmentation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PlpULkHdf6qt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7355
        },
        "outputId": "a8743379-e13b-4d1c-9bca-7384989c46cb"
      },
      "cell_type": "code",
      "source": [
        "model = DenseNet(img_dim, classes=nb_classes, depth=depth, nb_dense_block=nb_dense_block,\n",
        "                          growth_rate=growth_rate, nb_filter=nb_filter, dropout_rate=dropout_rate, weights=None)\n",
        "print(\"Model created\")\n",
        "\n",
        "model.summary()\n",
        "optimizer = SGD(lr=0.1,momentum=0.9,nesterov=True) # Using Adam instead of SGD to speed up training\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "print(\"Finished compiling\")\n",
        "print(\"Building model...\")\n",
        "\n",
        "(trainX, trainY), (testX, testY) = cifar10.load_data()\n",
        "\n",
        "trainX = trainX.astype('float32')\n",
        "testX = testX.astype('float32')\n",
        "\n",
        "trainX = preprocess_input(trainX)\n",
        "testX = preprocess_input(testX)\n",
        "\n",
        "Y_train = np_utils.to_categorical(trainY, nb_classes)\n",
        "Y_test = np_utils.to_categorical(testY, nb_classes)\n",
        "\n",
        "generator = ImageDataGenerator(rotation_range=15,\n",
        "                               width_shift_range=5./32,\n",
        "                               height_shift_range=5./32,\n",
        "                               horizontal_flip=True)\n",
        "\n",
        "generator.fit(trainX, seed=0)\n",
        "\n",
        "# Load model\n",
        "#weights_file=\"weights/DenseNet-40-12-CIFAR10.h5\"\n",
        "#if os.path.exists(weights_file):\n",
        "    #model.load_weights(weights_file, by_name=True)\n",
        "#    print(\"Model loaded.\")\n",
        "\n",
        "out_dir=\"weights/\"\n",
        "\n",
        "lr_reducer      = ReduceLROnPlateau(monitor='val_acc', factor=np.sqrt(0.1),\n",
        "                                    cooldown=0, patience=5, min_lr=1e-5)\n",
        "model_checkpoint= ModelCheckpoint( filepath,monitor=\"val_acc\", save_best_only=True,\n",
        "                                  save_weights_only=True, verbose=1)\n",
        "\n",
        "callbacks=[lr_reducer,model_checkpoint]\n",
        "\n",
        "model.fit_generator(generator.flow(trainX, Y_train, batch_size=batch_size),\n",
        "                    steps_per_epoch=len(trainX) // batch_size, epochs=nb_epoch,\n",
        "                    callbacks=callbacks,\n",
        "                    validation_data=(testX, Y_test),\n",
        "                    validation_steps=testX.shape[0] // batch_size, verbose=1)\n",
        "\n",
        "yPreds = model.predict(testX)\n",
        "yPred = np.argmax(yPreds, axis=1)\n",
        "yTrue = testY\n",
        "\n",
        "accuracy = metrics.accuracy_score(yTrue, yPred) * 100\n",
        "error = 100 - accuracy\n",
        "print(\"Accuracy : \", accuracy)\n",
        "print(\"Error : \", error)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model created\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_397 (Conv2D)             (None, 32, 32, 1)    27          input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_397 (BatchN (None, 32, 32, 1)    4           conv2d_397[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_397 (Activation)     (None, 32, 32, 1)    0           batch_normalization_397[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_398 (Conv2D)             (None, 32, 32, 12)   108         activation_397[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_193 (Concatenate)   (None, 32, 32, 13)   0           conv2d_397[0][0]                 \n",
            "                                                                 conv2d_398[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_398 (BatchN (None, 32, 32, 13)   52          concatenate_193[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_398 (Activation)     (None, 32, 32, 13)   0           batch_normalization_398[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_399 (Conv2D)             (None, 32, 32, 12)   1404        activation_398[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_194 (Concatenate)   (None, 32, 32, 25)   0           concatenate_193[0][0]            \n",
            "                                                                 conv2d_399[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_399 (BatchN (None, 32, 32, 25)   100         concatenate_194[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_399 (Activation)     (None, 32, 32, 25)   0           batch_normalization_399[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_400 (Conv2D)             (None, 32, 32, 12)   2700        activation_399[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_195 (Concatenate)   (None, 32, 32, 37)   0           concatenate_194[0][0]            \n",
            "                                                                 conv2d_400[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_400 (BatchN (None, 32, 32, 37)   148         concatenate_195[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_400 (Activation)     (None, 32, 32, 37)   0           batch_normalization_400[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_401 (Conv2D)             (None, 32, 32, 12)   3996        activation_400[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_196 (Concatenate)   (None, 32, 32, 49)   0           concatenate_195[0][0]            \n",
            "                                                                 conv2d_401[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_401 (BatchN (None, 32, 32, 49)   196         concatenate_196[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_401 (Activation)     (None, 32, 32, 49)   0           batch_normalization_401[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_402 (Conv2D)             (None, 32, 32, 12)   5292        activation_401[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_197 (Concatenate)   (None, 32, 32, 61)   0           concatenate_196[0][0]            \n",
            "                                                                 conv2d_402[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_402 (BatchN (None, 32, 32, 61)   244         concatenate_197[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_402 (Activation)     (None, 32, 32, 61)   0           batch_normalization_402[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_403 (Conv2D)             (None, 32, 32, 12)   6588        activation_402[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_198 (Concatenate)   (None, 32, 32, 73)   0           concatenate_197[0][0]            \n",
            "                                                                 conv2d_403[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_403 (BatchN (None, 32, 32, 73)   292         concatenate_198[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_403 (Activation)     (None, 32, 32, 73)   0           batch_normalization_403[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_404 (Conv2D)             (None, 32, 32, 12)   7884        activation_403[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_199 (Concatenate)   (None, 32, 32, 85)   0           concatenate_198[0][0]            \n",
            "                                                                 conv2d_404[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_404 (BatchN (None, 32, 32, 85)   340         concatenate_199[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_404 (Activation)     (None, 32, 32, 85)   0           batch_normalization_404[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_405 (Conv2D)             (None, 32, 32, 12)   9180        activation_404[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_200 (Concatenate)   (None, 32, 32, 97)   0           concatenate_199[0][0]            \n",
            "                                                                 conv2d_405[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_405 (BatchN (None, 32, 32, 97)   388         concatenate_200[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_405 (Activation)     (None, 32, 32, 97)   0           batch_normalization_405[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_406 (Conv2D)             (None, 32, 32, 12)   10476       activation_405[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_201 (Concatenate)   (None, 32, 32, 109)  0           concatenate_200[0][0]            \n",
            "                                                                 conv2d_406[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_406 (BatchN (None, 32, 32, 109)  436         concatenate_201[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_406 (Activation)     (None, 32, 32, 109)  0           batch_normalization_406[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_407 (Conv2D)             (None, 32, 32, 12)   11772       activation_406[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_202 (Concatenate)   (None, 32, 32, 121)  0           concatenate_201[0][0]            \n",
            "                                                                 conv2d_407[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_407 (BatchN (None, 32, 32, 121)  484         concatenate_202[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_407 (Activation)     (None, 32, 32, 121)  0           batch_normalization_407[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_408 (Conv2D)             (None, 32, 32, 12)   13068       activation_407[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_203 (Concatenate)   (None, 32, 32, 133)  0           concatenate_202[0][0]            \n",
            "                                                                 conv2d_408[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_408 (BatchN (None, 32, 32, 133)  532         concatenate_203[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_408 (Activation)     (None, 32, 32, 133)  0           batch_normalization_408[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_409 (Conv2D)             (None, 32, 32, 12)   14364       activation_408[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_204 (Concatenate)   (None, 32, 32, 145)  0           concatenate_203[0][0]            \n",
            "                                                                 conv2d_409[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_409 (BatchN (None, 32, 32, 145)  580         concatenate_204[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_409 (Activation)     (None, 32, 32, 145)  0           batch_normalization_409[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_410 (Conv2D)             (None, 32, 32, 145)  21025       activation_409[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_9 (AveragePoo (None, 16, 16, 145)  0           conv2d_410[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_410 (BatchN (None, 16, 16, 145)  580         average_pooling2d_9[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_410 (Activation)     (None, 16, 16, 145)  0           batch_normalization_410[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_411 (Conv2D)             (None, 16, 16, 12)   15660       activation_410[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_205 (Concatenate)   (None, 16, 16, 157)  0           average_pooling2d_9[0][0]        \n",
            "                                                                 conv2d_411[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_411 (BatchN (None, 16, 16, 157)  628         concatenate_205[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_411 (Activation)     (None, 16, 16, 157)  0           batch_normalization_411[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_412 (Conv2D)             (None, 16, 16, 12)   16956       activation_411[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_206 (Concatenate)   (None, 16, 16, 169)  0           concatenate_205[0][0]            \n",
            "                                                                 conv2d_412[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_412 (BatchN (None, 16, 16, 169)  676         concatenate_206[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_412 (Activation)     (None, 16, 16, 169)  0           batch_normalization_412[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_413 (Conv2D)             (None, 16, 16, 12)   18252       activation_412[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_207 (Concatenate)   (None, 16, 16, 181)  0           concatenate_206[0][0]            \n",
            "                                                                 conv2d_413[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_413 (BatchN (None, 16, 16, 181)  724         concatenate_207[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_413 (Activation)     (None, 16, 16, 181)  0           batch_normalization_413[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_414 (Conv2D)             (None, 16, 16, 12)   19548       activation_413[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_208 (Concatenate)   (None, 16, 16, 193)  0           concatenate_207[0][0]            \n",
            "                                                                 conv2d_414[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_414 (BatchN (None, 16, 16, 193)  772         concatenate_208[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_414 (Activation)     (None, 16, 16, 193)  0           batch_normalization_414[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_415 (Conv2D)             (None, 16, 16, 12)   20844       activation_414[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_209 (Concatenate)   (None, 16, 16, 205)  0           concatenate_208[0][0]            \n",
            "                                                                 conv2d_415[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_415 (BatchN (None, 16, 16, 205)  820         concatenate_209[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_415 (Activation)     (None, 16, 16, 205)  0           batch_normalization_415[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_416 (Conv2D)             (None, 16, 16, 12)   22140       activation_415[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_210 (Concatenate)   (None, 16, 16, 217)  0           concatenate_209[0][0]            \n",
            "                                                                 conv2d_416[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_416 (BatchN (None, 16, 16, 217)  868         concatenate_210[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_416 (Activation)     (None, 16, 16, 217)  0           batch_normalization_416[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_417 (Conv2D)             (None, 16, 16, 12)   23436       activation_416[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_211 (Concatenate)   (None, 16, 16, 229)  0           concatenate_210[0][0]            \n",
            "                                                                 conv2d_417[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_417 (BatchN (None, 16, 16, 229)  916         concatenate_211[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_417 (Activation)     (None, 16, 16, 229)  0           batch_normalization_417[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_418 (Conv2D)             (None, 16, 16, 12)   24732       activation_417[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_212 (Concatenate)   (None, 16, 16, 241)  0           concatenate_211[0][0]            \n",
            "                                                                 conv2d_418[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_418 (BatchN (None, 16, 16, 241)  964         concatenate_212[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_418 (Activation)     (None, 16, 16, 241)  0           batch_normalization_418[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_419 (Conv2D)             (None, 16, 16, 12)   26028       activation_418[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_213 (Concatenate)   (None, 16, 16, 253)  0           concatenate_212[0][0]            \n",
            "                                                                 conv2d_419[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_419 (BatchN (None, 16, 16, 253)  1012        concatenate_213[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_419 (Activation)     (None, 16, 16, 253)  0           batch_normalization_419[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_420 (Conv2D)             (None, 16, 16, 12)   27324       activation_419[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_214 (Concatenate)   (None, 16, 16, 265)  0           concatenate_213[0][0]            \n",
            "                                                                 conv2d_420[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_420 (BatchN (None, 16, 16, 265)  1060        concatenate_214[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_420 (Activation)     (None, 16, 16, 265)  0           batch_normalization_420[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_421 (Conv2D)             (None, 16, 16, 12)   28620       activation_420[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_215 (Concatenate)   (None, 16, 16, 277)  0           concatenate_214[0][0]            \n",
            "                                                                 conv2d_421[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_421 (BatchN (None, 16, 16, 277)  1108        concatenate_215[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_421 (Activation)     (None, 16, 16, 277)  0           batch_normalization_421[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_422 (Conv2D)             (None, 16, 16, 12)   29916       activation_421[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_216 (Concatenate)   (None, 16, 16, 289)  0           concatenate_215[0][0]            \n",
            "                                                                 conv2d_422[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_422 (BatchN (None, 16, 16, 289)  1156        concatenate_216[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_422 (Activation)     (None, 16, 16, 289)  0           batch_normalization_422[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_423 (Conv2D)             (None, 16, 16, 289)  83521       activation_422[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_10 (AveragePo (None, 8, 8, 289)    0           conv2d_423[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_423 (BatchN (None, 8, 8, 289)    1156        average_pooling2d_10[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_423 (Activation)     (None, 8, 8, 289)    0           batch_normalization_423[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_424 (Conv2D)             (None, 8, 8, 12)     31212       activation_423[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_217 (Concatenate)   (None, 8, 8, 301)    0           average_pooling2d_10[0][0]       \n",
            "                                                                 conv2d_424[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_424 (BatchN (None, 8, 8, 301)    1204        concatenate_217[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_424 (Activation)     (None, 8, 8, 301)    0           batch_normalization_424[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_425 (Conv2D)             (None, 8, 8, 12)     32508       activation_424[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_218 (Concatenate)   (None, 8, 8, 313)    0           concatenate_217[0][0]            \n",
            "                                                                 conv2d_425[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_425 (BatchN (None, 8, 8, 313)    1252        concatenate_218[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_425 (Activation)     (None, 8, 8, 313)    0           batch_normalization_425[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_426 (Conv2D)             (None, 8, 8, 12)     33804       activation_425[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_219 (Concatenate)   (None, 8, 8, 325)    0           concatenate_218[0][0]            \n",
            "                                                                 conv2d_426[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_426 (BatchN (None, 8, 8, 325)    1300        concatenate_219[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_426 (Activation)     (None, 8, 8, 325)    0           batch_normalization_426[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_427 (Conv2D)             (None, 8, 8, 12)     35100       activation_426[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_220 (Concatenate)   (None, 8, 8, 337)    0           concatenate_219[0][0]            \n",
            "                                                                 conv2d_427[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_427 (BatchN (None, 8, 8, 337)    1348        concatenate_220[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_427 (Activation)     (None, 8, 8, 337)    0           batch_normalization_427[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_428 (Conv2D)             (None, 8, 8, 12)     36396       activation_427[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_221 (Concatenate)   (None, 8, 8, 349)    0           concatenate_220[0][0]            \n",
            "                                                                 conv2d_428[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_428 (BatchN (None, 8, 8, 349)    1396        concatenate_221[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_428 (Activation)     (None, 8, 8, 349)    0           batch_normalization_428[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_429 (Conv2D)             (None, 8, 8, 12)     37692       activation_428[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_222 (Concatenate)   (None, 8, 8, 361)    0           concatenate_221[0][0]            \n",
            "                                                                 conv2d_429[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_429 (BatchN (None, 8, 8, 361)    1444        concatenate_222[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_429 (Activation)     (None, 8, 8, 361)    0           batch_normalization_429[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_430 (Conv2D)             (None, 8, 8, 12)     38988       activation_429[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_223 (Concatenate)   (None, 8, 8, 373)    0           concatenate_222[0][0]            \n",
            "                                                                 conv2d_430[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_430 (BatchN (None, 8, 8, 373)    1492        concatenate_223[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_430 (Activation)     (None, 8, 8, 373)    0           batch_normalization_430[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_431 (Conv2D)             (None, 8, 8, 12)     40284       activation_430[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_224 (Concatenate)   (None, 8, 8, 385)    0           concatenate_223[0][0]            \n",
            "                                                                 conv2d_431[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_431 (BatchN (None, 8, 8, 385)    1540        concatenate_224[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_431 (Activation)     (None, 8, 8, 385)    0           batch_normalization_431[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_432 (Conv2D)             (None, 8, 8, 12)     41580       activation_431[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_225 (Concatenate)   (None, 8, 8, 397)    0           concatenate_224[0][0]            \n",
            "                                                                 conv2d_432[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_432 (BatchN (None, 8, 8, 397)    1588        concatenate_225[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_432 (Activation)     (None, 8, 8, 397)    0           batch_normalization_432[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_433 (Conv2D)             (None, 8, 8, 12)     42876       activation_432[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_226 (Concatenate)   (None, 8, 8, 409)    0           concatenate_225[0][0]            \n",
            "                                                                 conv2d_433[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_433 (BatchN (None, 8, 8, 409)    1636        concatenate_226[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_433 (Activation)     (None, 8, 8, 409)    0           batch_normalization_433[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_434 (Conv2D)             (None, 8, 8, 12)     44172       activation_433[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_227 (Concatenate)   (None, 8, 8, 421)    0           concatenate_226[0][0]            \n",
            "                                                                 conv2d_434[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_434 (BatchN (None, 8, 8, 421)    1684        concatenate_227[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_434 (Activation)     (None, 8, 8, 421)    0           batch_normalization_434[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_435 (Conv2D)             (None, 8, 8, 12)     45468       activation_434[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_228 (Concatenate)   (None, 8, 8, 433)    0           concatenate_227[0][0]            \n",
            "                                                                 conv2d_435[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_435 (BatchN (None, 8, 8, 433)    1732        concatenate_228[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_435 (Activation)     (None, 8, 8, 433)    0           batch_normalization_435[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_5 (Glo (None, 433)          0           activation_435[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 10)           4340        global_average_pooling2d_5[0][0] \n",
            "==================================================================================================\n",
            "Total params: 963,133\n",
            "Trainable params: 946,207\n",
            "Non-trainable params: 16,926\n",
            "__________________________________________________________________________________________________\n",
            "Finished compiling\n",
            "Building model...\n",
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 22s 0us/step\n",
            "Epoch 1/300\n",
            "781/781 [==============================] - 269s 344ms/step - loss: 1.5470 - acc: 0.4647 - val_loss: 2.0001 - val_acc: 0.4788\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.47880, saving model to /content/gdrive/My Drive/model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-237d6303f385>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                     validation_steps=testX.shape[0] // batch_size, verbose=1)\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0myPreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2266\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2269\u001b[0m                 \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    443\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite)\u001b[0m\n\u001b[1;32m   2619\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mproceed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2620\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2621\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2622\u001b[0m             \u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2623\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to open file: name = '/content/gdrive/My Drive/model', errno = 21, error message = 'Is a directory', flags = 13, o_flags = 242)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "F-ttLRlEyEYO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"def __create_fcn_dense_net(nb_classes, img_input, include_top, nb_dense_block=5, growth_rate=12,\n",
        "                           reduction=0.0, dropout_rate=None, weight_decay=1e-4,\n",
        "                           nb_layers_per_block=4, nb_upsampling_conv=128, upsampling_type='upsampling',\n",
        "                           init_conv_filters=48, input_shape=None, activation='deconv'):\n",
        "    ''' Build the DenseNet model\n",
        "    Args:\n",
        "        nb_classes: number of classes\n",
        "        img_input: tuple of shape (channels, rows, columns) or (rows, columns, channels)\n",
        "        include_top: flag to include the final Dense layer\n",
        "        nb_dense_block: number of dense blocks to add to end (generally = 3)\n",
        "        growth_rate: number of filters to add per dense block\n",
        "        reduction: reduction factor of transition blocks. Note : reduction value is inverted to compute compression\n",
        "        dropout_rate: dropout rate\n",
        "        weight_decay: weight decay\n",
        "        nb_layers_per_block: number of layers in each dense block.\n",
        "            Can be a positive integer or a list.\n",
        "            If positive integer, a set number of layers per dense block.\n",
        "            If list, nb_layer is used as provided. Note that list size must\n",
        "            be (nb_dense_block + 1)\n",
        "        nb_upsampling_conv: number of convolutional layers in upsampling via subpixel convolution\n",
        "        upsampling_type: Can be one of 'upsampling', 'deconv' and 'subpixel'. Defines\n",
        "            type of upsampling algorithm used.\n",
        "        input_shape: Only used for shape inference in fully convolutional networks.\n",
        "        activation: Type of activation at the top layer. Can be one of 'softmax' or 'sigmoid'.\n",
        "                    Note that if sigmoid is used, classes must be 1.\n",
        "    Returns: keras tensor with nb_layers of conv_block appended\n",
        "    '''\n",
        "\n",
        "    concat_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
        "\n",
        "    if concat_axis == 1:  # channels_first dim ordering\n",
        "        _, rows, cols = input_shape\n",
        "    else:\n",
        "        rows, cols, _ = input_shape\n",
        "\n",
        "    if reduction != 0.0:\n",
        "        assert reduction <= 1.0 and reduction > 0.0, 'reduction value must lie between 0.0 and 1.0'\n",
        "\n",
        "    # check if upsampling_conv has minimum number of filters\n",
        "    # minimum is set to 12, as at least 3 color channels are needed for correct upsampling\n",
        "    assert nb_upsampling_conv > 12 and nb_upsampling_conv % 4 == 0, 'Parameter `upsampling_conv` number of channels must ' \\\n",
        "                                                                    'be a positive number divisible by 4 and greater ' \\\n",
        "                                                                    'than 12'\n",
        "\n",
        "    # layers in each dense block\n",
        "    if type(nb_layers_per_block) is list or type(nb_layers_per_block) is tuple:\n",
        "        nb_layers = list(nb_layers_per_block)  # Convert tuple to list\n",
        "\n",
        "        assert len(nb_layers) == (nb_dense_block + 1), 'If list, nb_layer is used as provided. ' \\\n",
        "                                                       'Note that list size must be (nb_dense_block + 1)'\n",
        "\n",
        "        bottleneck_nb_layers = nb_layers[-1]\n",
        "        rev_layers = nb_layers[::-1]\n",
        "        nb_layers.extend(rev_layers[1:])\n",
        "    else:\n",
        "        bottleneck_nb_layers = nb_layers_per_block\n",
        "        nb_layers = [nb_layers_per_block] * (2 * nb_dense_block + 1)\n",
        "\n",
        "    # compute compression factor\n",
        "    compression = 1.0 - reduction\n",
        "\n",
        "    # Initial convolution\n",
        "    x = Conv2D(init_conv_filters, (7, 7), kernel_initializer='he_normal', padding='same', name='initial_conv2D',\n",
        "               use_bias=False, kernel_regularizer=l2(weight_decay))(img_input)\n",
        "    x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    nb_filter = init_conv_filters\n",
        "\n",
        "    skip_list = []\n",
        "\n",
        "    # Add dense blocks and transition down block\n",
        "    for block_idx in range(nb_dense_block):\n",
        "        x, nb_filter = __dense_block(x, nb_layers[block_idx], nb_filter, growth_rate, dropout_rate=dropout_rate,\n",
        "                                     weight_decay=weight_decay)\n",
        "\n",
        "        # Skip connection\n",
        "        skip_list.append(x)\n",
        "\n",
        "        # add transition_block\n",
        "        x = __transition_block(x, nb_filter, compression=compression, weight_decay=weight_decay)\n",
        "\n",
        "        nb_filter = int(nb_filter * compression)  # this is calculated inside transition_down_block\n",
        "\n",
        "    # The last dense_block does not have a transition_down_block\n",
        "    # return the concatenated feature maps without the concatenation of the input\n",
        "    _, nb_filter, concat_list = __dense_block(x, bottleneck_nb_layers, nb_filter, growth_rate,\n",
        "                                              dropout_rate=dropout_rate, weight_decay=weight_decay,\n",
        "                                              return_concat_list=True)\n",
        "\n",
        "    skip_list = skip_list[::-1]  # reverse the skip list\n",
        "\n",
        "    # Add dense blocks and transition up block\n",
        "    for block_idx in range(nb_dense_block):\n",
        "        n_filters_keep = growth_rate * nb_layers[nb_dense_block + block_idx]\n",
        "\n",
        "        # upsampling block must upsample only the feature maps (concat_list[1:]),\n",
        "        # not the concatenation of the input with the feature maps (concat_list[0].\n",
        "        l = concatenate(concat_list[1:], axis=concat_axis)\n",
        "\n",
        "        t = __transition_up_block(l, nb_filters=n_filters_keep, type=upsampling_type, weight_decay=weight_decay)\n",
        "\n",
        "        # concatenate the skip connection with the transition block\n",
        "        x = concatenate([t, skip_list[block_idx]], axis=concat_axis)\n",
        "\n",
        "        # Dont allow the feature map size to grow in upsampling dense blocks\n",
        "        x_up, nb_filter, concat_list = __dense_block(x, nb_layers[nb_dense_block + block_idx + 1], nb_filter=growth_rate,\n",
        "                                                     growth_rate=growth_rate, dropout_rate=dropout_rate,\n",
        "                                                     weight_decay=weight_decay, return_concat_list=True,\n",
        "                                                     grow_nb_filters=False)\n",
        "\n",
        "    if include_top:\n",
        "        x = Conv2D(nb_classes, (1, 1), activation='linear', padding='same', use_bias=False)(x_up)\n",
        "\n",
        "        if K.image_data_format() == 'channels_first':\n",
        "            channel, row, col = input_shape\n",
        "        else:\n",
        "            row, col, channel = input_shape\n",
        "\n",
        "        x = Reshape((row * col, nb_classes))(x)\n",
        "        x = Activation(activation)(x)\n",
        "        x = Reshape((row, col, nb_classes))(x)\n",
        "    else:\n",
        "        x = x_up\n",
        "\n",
        "    return x\"\"\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nqyQgmbcyI5f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"def DenseNetFCN(input_shape, nb_dense_block=5, growth_rate=16, nb_layers_per_block=4,\n",
        "                reduction=0.0, dropout_rate=0.0, weight_decay=1e-4, init_conv_filters=48,\n",
        "                include_top=True, weights=None, input_tensor=None, classes=1, activation='softmax',\n",
        "                upsampling_conv=128, upsampling_type='deconv'):\n",
        "    '''Instantiate the DenseNet FCN architecture.\n",
        "        Note that when using TensorFlow,\n",
        "        for best performance you should set\n",
        "        `image_data_format='channels_last'` in your Keras config\n",
        "        at ~/.keras/keras.json.\n",
        "        # Arguments\n",
        "            nb_dense_block: number of dense blocks to add to end (generally = 3)\n",
        "            growth_rate: number of filters to add per dense block\n",
        "            nb_layers_per_block: number of layers in each dense block.\n",
        "                Can be a positive integer or a list.\n",
        "                If positive integer, a set number of layers per dense block.\n",
        "                If list, nb_layer is used as provided. Note that list size must\n",
        "                be (nb_dense_block + 1)\n",
        "            reduction: reduction factor of transition blocks.\n",
        "                Note : reduction value is inverted to compute compression.\n",
        "            dropout_rate: dropout rate\n",
        "            init_conv_filters: number of layers in the initial convolution layer\n",
        "            include_top: whether to include the fully-connected\n",
        "                layer at the top of the network.\n",
        "            weights: one of `None` (random initialization) or\n",
        "                'cifar10' (pre-training on CIFAR-10)..\n",
        "            input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n",
        "                to use as image input for the model.\n",
        "            input_shape: optional shape tuple, only to be specified\n",
        "                if `include_top` is False (otherwise the input shape\n",
        "                has to be `(32, 32, 3)` (with `channels_last` dim ordering)\n",
        "                or `(3, 32, 32)` (with `channels_first` dim ordering).\n",
        "                It should have exactly 3 inputs channels,\n",
        "                and width and height should be no smaller than 8.\n",
        "                E.g. `(200, 200, 3)` would be one valid value.\n",
        "            classes: optional number of classes to classify images\n",
        "                into, only to be specified if `include_top` is True, and\n",
        "                if no `weights` argument is specified.\n",
        "            activation: Type of activation at the top layer. Can be one of 'softmax' or 'sigmoid'.\n",
        "                Note that if sigmoid is used, classes must be 1.\n",
        "            upsampling_conv: number of convolutional layers in upsampling via subpixel convolution\n",
        "            upsampling_type: Can be one of 'upsampling', 'deconv' and\n",
        "                'subpixel'. Defines type of upsampling algorithm used.\n",
        "            batchsize: Fixed batch size. This is a temporary requirement for\n",
        "                computation of output shape in the case of Deconvolution2D layers.\n",
        "                Parameter will be removed in next iteration of Keras, which infers\n",
        "                output shape of deconvolution layers automatically.\n",
        "        # Returns\n",
        "            A Keras model instance.\n",
        "    '''\n",
        "\n",
        "    if weights not in {None}:\n",
        "        raise ValueError('The `weights` argument should be '\n",
        "                         '`None` (random initialization) as no '\n",
        "                         'model weights are provided.')\n",
        "\n",
        "    upsampling_type = upsampling_type.lower()\n",
        "\n",
        "    if upsampling_type not in ['upsampling', 'deconv', 'subpixel']:\n",
        "        raise ValueError('Parameter \"upsampling_type\" must be one of \"upsampling\", '\n",
        "                         '\"deconv\" or \"subpixel\".')\n",
        "\n",
        "    if input_shape is None:\n",
        "        raise ValueError('For fully convolutional models, input shape must be supplied.')\n",
        "\n",
        "    if type(nb_layers_per_block) is not list and nb_dense_block < 1:\n",
        "        raise ValueError('Number of dense layers per block must be greater than 1. Argument '\n",
        "                         'value was %d.' % (nb_layers_per_block))\n",
        "\n",
        "    if activation not in ['softmax', 'sigmoid']:\n",
        "        raise ValueError('activation must be one of \"softmax\" or \"sigmoid\"')\n",
        "\n",
        "    if activation == 'sigmoid' and classes != 1:\n",
        "        raise ValueError('sigmoid activation can only be used when classes = 1')\n",
        "\n",
        "    # Determine proper input shape\n",
        "    min_size = 2 ** nb_dense_block\n",
        "\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        if input_shape is not None:\n",
        "            if ((input_shape[1] is not None and input_shape[1] < min_size) or\n",
        "                    (input_shape[2] is not None and input_shape[2] < min_size)):\n",
        "                raise ValueError('Input size must be at least ' +\n",
        "                                 str(min_size) + 'x' + str(min_size) + ', got '\n",
        "                                                                       '`input_shape=' + str(input_shape) + '`')\n",
        "        else:\n",
        "            input_shape = (classes, None, None)\n",
        "    else:\n",
        "        if input_shape is not None:\n",
        "            if ((input_shape[0] is not None and input_shape[0] < min_size) or\n",
        "                    (input_shape[1] is not None and input_shape[1] < min_size)):\n",
        "                raise ValueError('Input size must be at least ' +\n",
        "                                 str(min_size) + 'x' + str(min_size) + ', got '\n",
        "                                                                       '`input_shape=' + str(input_shape) + '`')\n",
        "        else:\n",
        "            input_shape = (None, None, classes)\n",
        "\n",
        "    if input_tensor is None:\n",
        "        img_input = Input(shape=input_shape)\n",
        "    else:\n",
        "        if not K.is_keras_tensor(input_tensor):\n",
        "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
        "        else:\n",
        "            img_input = input_tensor\n",
        "\n",
        "    x = __create_fcn_dense_net(classes, img_input, include_top, nb_dense_block,\n",
        "                               growth_rate, reduction, dropout_rate, weight_decay,\n",
        "                               nb_layers_per_block, upsampling_conv, upsampling_type,\n",
        "                               init_conv_filters, input_shape, activation)\n",
        "\n",
        "    # Ensure that the model takes into account\n",
        "    # any potential predecessors of `input_tensor`.\n",
        "    if input_tensor is not None:\n",
        "        inputs = get_source_inputs(input_tensor)\n",
        "    else:\n",
        "        inputs = img_input\n",
        "    # Create model.\n",
        "    model = Model(inputs, x, name='fcn-densenet')\n",
        "\n",
        "    return model\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}